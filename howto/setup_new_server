in aws web console
  - make 3 new ec2 elastic block storage (ebs) instances
      Web Dev 1 A - fulcrum - repos     (drupal core code)
      Web Dev 1 A - fulcrum - files     (drupal uploads)
      Web Dev 1 A - fulcrum - data      (databases)
  - tst and dev are typically put in zone A, but check the ec2 instances location
  *** note that tst is a combo of train and tst, so these volumes should be twice as large as the dev ones
  *** also note that in prod we are using EBS so this may be not be the same steps

  - attach volumes to ec2 instance, make note of the xvd* that each is mounted as
    do them in the order of
    data
    repos
    files


ssh into system, you may have to whitelist your ip via ssh to the system(s)

  # you can use ls -l /dev/xv*, and detach/reattach to make sure what your target is, is correct
  - set ext4 for the new volumes
    sudo mkfs.ext4 /dev/xvdj
    sudo mkfs.ext4 /dev/xvdk
    sudo mkfs.ext4 /dev/xvdl

  - mnt new volumes
    sudo mkdir -p /mnt/data /mnt/files /mnt/repos
    sudo mount /dev/xvdj /mnt/data
    sudo mount /dev/xvdk /mnt/repos
    sudo mount /dev/xvdl /mnt/files

  - verify the volumes are named correctly by comparing the size of the volumes
    df -h

  - shut down old fulcrum (mostly so mysql isn't writing while we copy)
    sudo /usr/local/bin/docker-compose --file ~/fulcrum/compose/fulcrum-dev-stack.yml stop
    # NOTE that the above command may be a different file path
    # sudo /usr/local/bin/docker-compose --file ~/fulcrum/compose/tst/fulcrum-stack.yml stop
    # sudo /usr/local/bin/docker-compose --file ~/fulcrum/compose/prd/fulcrum-stack.yml stop

  - copy over existing data
    sudo cp -a /usr/local/fulcrum/db/mysql /mnt/data/
    sudo cp -a /usr/local/fulcrum/files/* /mnt/files/
    sudo cp -a /usr/local/fulcrum/webroots/* /mnt/repos/

  - make a copy/backup of a few items from the old fulcrum-env
    sudo mkdir /usr/local/fulcrum.bak.20160225
    sudo mv /usr/local/fulcrum/etc /usr/local/fulcrum.bak.20160225
    sudo mv /usr/local/fulcrum/bin /usr/local/fulcrum.bak.20160225
    sudo mv /usr/local/fulcrum/conf /usr/local/fulcrum.bak.20160225
    sudo mv /usr/local/fulcrum/php /usr/local/fulcrum.bak.20160225
    # NOTE you should look at the other dirs in local/fulcrum and see if you think you might need any of the others.. most of the others you copied to the /mnts above

  - for prod, we had some extra sylinks in usr local fulcrum
    - files -> /sfs/files
    - tmp -> /sfs/other/tmp
    # becuase we ended up moving all the real dirs out of fulcrum and into the bak folder, we didn't have to remake links

  - umount the old fulcrum
    # NOTE on prod, these were not mounts but part of the local file system
    sudo umount /usr/local/fulcrum/db
    sudo umount /usr/local/fulcrum/

  - clone git based fulcrum-env, archive old one
    # check that /home/fulcrum/.ssh has valid .id_rsa that is good in code commit
    sudo chown fulcrum.fulcrum /usr/local/fulcrum
    sudo -s  # become root
    su - fulcrum -s /bin/bash  # become the fulcrum user with A SHELL, else you are in the odd github chroot shell thing
    vi ~/.ssh/config
    chmod 600 ~/.ssh/config
    cd /usr/local/fulcrum
    git clone ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/fulcrum-env .
    exit  # back to root
    exit  # back to centos user

  - selinux fix else fulcrum won't come back up
    sudo setenforce 0
    sudo chcon -Rt svirt_sandbox_file_t /usr/local/fulcrum/log
    sudo chcon -Rt svirt_sandbox_file_t /usr/local/fulcrum/etc
    sudo chcon -Rt svirt_sandbox_file_t /usr/local/fulcrum/php

  - create the new repos mount point
    sudo mkdir /usr/local/repos


  - mount the new volumes to the correct locations... missed this on dev/tst so only have prod repo here
  - manually mount the mounts to their final place (no /etc/fstab yet)
    sudo umount /mnt/repos/
    sudo rmdir /mnt/repos/
    sudo mount /dev/xvdh /usr/local/repos/

  - remove stock conf settings and add existing ones
    rm /usr/local/fulcrum/conf/*.json
    cp /usr/local/fulcrum.bak.20160225/conf/* /usr/local/fulcrum/conf
    sudo chown -R fulcrum.fulcrum /usr/local/fulcrum/conf

  - fix some other perms
    sudo chown fulcrum.fulfcrum /usr/local/fulcrum/data
    sudo chown fulcrum.fulfcrum /usr/local/fulcrum/files

  - in webroots, it should be the final "reallish" url, that you are symlinking to
    [centos@web01-tst-noc1.ifshr webroots 14:26]$ ln -s ../../repos/test.jaxpubliclibrary.org test.jaxpubliclibrary.ifdemo.net

  - make the new nginx conf files
    node ~/fulcrum/bin/fulcrumConfs.js ~/fulcrum/conf > ~/fulcrum/etc/nginx/fulcrum_config.conf
    node ~/fulcrum/bin/fulcrumEnvs.js ~/fulcrum/conf > ~/fulcrum/etc/nginx/fulcrum_env.conf
    node ~/fulcrum/bin/fulcrumWebroots.js ~/fulcrum/conf > ~/fulcrum/etc/nginx/fulcrum_webroot.conf

  - use the compose tst instead of dev stack # we made this one manually but it was very similar to dev
    ~/fulcrum/compose/

  - see if fulcrum will start
    # for whatever reason crap was not responding to docker-compose, so i had to docker stop/rm by hand
    ## sudo docker ps -a | awk '$1 !~ /CONT/ { print "sudo docker stop "$1"; sudo docker rm "$1 }' | bash
    sudo /usr/local/bin/docker-compose --file ~/fulcrum/compose/fulcrum-dev-stack.yml stop
    sudo docker ps   # see if we have existing containors


  - update docker images, start fulcrum with compose
    sudo /usr/local/bin/docker-compose --file ~/fulcrum/compose/fulcrum-dev-stack.yml pull
    sudo /usr/local/bin/docker-compose --file ~/fulcrum/compose/fulcrum-dev-stack.yml up -d
    sudo /usr/local/bin/docker-compose --file ~/fulcrum/compose/fulcrum-dev-stack.yml ps # look for restarts

  - remove entries in fstab, so we can halt system and remove the old volumes
    vi /etc/fstab
    /dev/xvdg /usr/local/fulcrum/data  ext4    defaults    0 2
    /dev/xvdh /usr/local/repos         ext4    defaults        0 2
    /dev/xvdi /usr/local/fulcrum/files ext4    defaults        0 2

  - shutdown system and unattach old volumes
    sudo shutdown -h now
    # aws web ui, ec2 > ensure stopped
    # aws web ui, instances > ensure old volumes are not attached

  - boot back up, see if you can start fulcrum


  - fix fstab, umount drives and then mount with fstab, restart compose





Enable redis+varnish super speedy cache

   - You will need to install the following 3 modules (check they maybe already be installed
     https://www.drupal.org/project/varnish
     https://www.drupal.org/project/redis
     https://www.drupal.org/project/expire

   - THERE ARE 2 TRACKS, one for using aws elastic-cache (prod), the other for dev/test with container redis

   - dev/test/train
     - enable setting in json ~/fulcrum/conf
     - login to back office by browser (see below for this with drush uli)
       - go to modules
         - enable the 3 modules, save at bottom
         - configure expire module > External Expire... rest default
         - redis > Host = redis2-8; Client = PhpRedis PHP extension
         - varnish > Varnish Version = 3.x; Varnish Control Terminal=varnish3-0:6082; Varnish Cache Clearing=Selective


   - prod
     - enable elastic-cache in the backend of AWS (already setup so no notes for this one... see SD as example)
       - to get the actual end point, go to AWS > ElastiCache > Replication Groups > Click group > Below "Primary EndPoint"
     - login to back office by browser (see below for this with drush uli)
         - enable the 3 modules, save at bottom
         # NOW IN nginx json config !! - configure expire module > External Expire... rest default
         # NOW IN nginx json config !! - redis > Host = redis2-8; Client = PhpRedis PHP extension
         # NOW IN nginx json config !! - varnish > Varnish Version = 3.x; Varnish Control Terminal=varnish3-0:6082; Varnish Cache Clearing=Selective


    - check out the sites conf file for nginx
      # create the drupal salt:
      openssl rand -base64 8 | gsha256sum # on mac

# this is how you generate a login to drupal back office
# sudo ~/bin/drush www.jaxpubliclibrary.ifsight.net uli  # on google sheet


# had to fix some of the naming in webroots as we moved away from the demo name
# Error response from daemon: Cannot start container adac8d305ce1d50fab6d46029c228896014784d838360b8b96eca09758c0e4a4: [8] System error: no such file or directory
# above error is related to a containtor not finding symlinks or something, figure out the volmnes (drush set -x if running drush) to see what volme is missing

[centos@web01-dev-noc1.ifshr webroots 09:09]$ ls -d1 *ifdemo* | awk '{ o=$0; sub("ifdemo.net", "ifsight.net"); print "mv "o "   " $0 }'  | bash
[centos@web01-dev-noc1.ifshr webroots 09:45]$ ls -1d *ifsight.net* | awk '{ o=$0; sub("ifdemo", "ifsight"); print "rm "o"; ln -s ../../repos/"o }' | bash
